0. The docker client plugin for jenkins does not work on the openshift jenkins slaves out of the box. I also doesnt work with unix sockets. The only way to get it to work
is to configure the Docker daemon to expose itself on a TCP IP port, and then connect to it from the withServer method. TODO test if we can use 172.17.0.1
1. The Openshift Jenkins Sync Client plugin is a master-based dev project. It still seems to be pre-release status. Some unknown version of this plugin is deployed to Openshift.
Unfortunately that means that some of the features do not seem to be supported in the current version of Openshift. For example, we could not get the  Jenkins File Credentials
to sync from Openshift secrets.
2. There is no way of syncing Openshift Secrets to any of the Jenkins Openshift credential types. Those would be required for the openshift.withCluster method that is invoked
when we interact with any of the external clusters. In the end we had to opt for a token which is generated in the installation script. This is a massive security risk and
should be replaced manually by the provisioner as soon as possible with the id of an Jenkins Openshift Token Credential.
3. There is a known issue using Openshift Pull Secrets for the private repositories hosted on docker.io. This issue seems to come back every now and then in minor releases due to
regression problems. The solution (https://github.com/openshift/origin/issues/18932) is to add the docker.io regristry to /etc/sysconfig/docker:
ADD_REGISTRY='--add-registry docker.io --add-registry registry.access.redhat.com'
4. Customer Jenkins Image Streams have to be installed in the same project as Jenkins for Jenkins to pick them up and make them available.
5. Something strange is going on in openshift.serv.run around the automatic provisioning of PersistentVolumes. It doesn't always clear the PV when recycling it. If the PostgreSQL
image fails and complains about things like "a server already running" then it means it picked up a dirty PV. Go into /data/openshift/<pvid> and delete all files recursively
6. Openshift secrets aren't very secure. Jenkins credentials are more secure, but are difficult to impossible to automate. For more security, I would suggest manually creating
Credentials in Jenkins and deleting as many of the Secrets in Openshift.
7. The Openshift Multitenant Network Plugin blocks inter-project Pod communication. Wherever this plugin is installed, we need to join the build and deployment networks.
This is done in the installation script. It fails on Openshift clusters where the Multitenant Network Plugin is not installed, but that can be ignored.
8. ImageStreams add a lot of indirection that needs to be synced. You can uese Docker images directly from a DC.
9. The version of the Jenkins running in Openshift does not support nested stages yet.
10. Even though a pipeline build does not require a source code specification, Bitbucket (and probabluy other) webhooks don't work unless Openshift can find a BuildConfig
with a matching Source Specification
11. Our builds are still a bit on the slow side. We need to look at disabling some of the maven plugins and finding a faster way to restore the DB
12. Sometimes you get timeouts when interacting with docker.io. Fix this by adding 8.8.8.8 to the top of resolv.conf if it is not already there



TODO:
We should put entando-central's standalone-openshift.xml in the Imagick image. We don't need the datasources, but the handler for the resources and the caches are required.
We still need to automate the cert generation
Create a PostgreSQL image WITHOUT Maven and Ant that only populates the db using Java and Jetty
Test the DB backup/restore on failure properly
Make sure that all our bash scripts return non-zero on failure
Scale back up when the DB migration fails
Undo a rollout and DB migration when the final rollout fails
Make the actual image name configurable
Try to get rid of the production cluster secret and use only the token, which is generally interchangeable with the id of the Openshift Token Credential in Jenkins